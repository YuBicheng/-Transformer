{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca277795-7473-4e6d-b9fc-b10816ee2b80",
   "metadata": {},
   "source": [
    "# Transformer批量训练的代码及注释"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d097a9b9-8ce9-4c4f-871c-5c8f4d440959",
   "metadata": {},
   "source": [
    "将需要的库进行加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23333c8e-1602-4564-9d23-df1f7f6ddad9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#本地文件引入\n",
    "from Utilities import Mytokenizer\n",
    "import Transformer\n",
    "#库引入\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "\n",
    "#设置device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8dae8a-42d2-408a-856c-b0752bac399e",
   "metadata": {},
   "source": [
    "## Dataset类实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a94feb-3b76-4363-a2ff-1cc70d2e0bcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Mydataset(Dataset):\n",
    "    \"\"\"\n",
    "    @file_path 数据存储位置\n",
    "    @tokenizer 将文字id化的实例化后的Tokenizer\n",
    "    @文件中 trg数据的位置\n",
    "    \n",
    "    方便使用的dataset封装 详细注释及教学版请看 数据的全流程解释.ipynb\n",
    "    \"\"\"\n",
    "    def __init__(self,file_path,tokenizer,trg_index=0):\n",
    "        self.tokenizer = tokenizer\n",
    "        #读取所有文本\n",
    "        with open(file_path,'r',encoding='utf8') as f:\n",
    "            self.lines = f.readlines()\n",
    "        #self.trg_count_words_line = []\n",
    "        self.length = len(self.lines)\n",
    "        self.trg_index = trg_index\n",
    "        if trg_index==0:\n",
    "            self.src_index = 1\n",
    "        else:\n",
    "            self.src_index = 0\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        src = self.lines[index].split('\\t')[self.src_index]\n",
    "        src = src.split('\\n')[0]\n",
    "        #print(\"src:\",src)\n",
    "        trg = self.lines[index].split('\\t')[self.trg_index]\n",
    "        #print(\"trg:\",trg)\n",
    "        #如上面的例子 三个词的句子应有四个样本,所以应该拷贝三次\n",
    "        #copy_time = len(trg.split(' '))\n",
    "        #print(copy_time)\n",
    "        # src_id化 这里简单定义了src使用中文\n",
    "        src_id = self.tokenizer.ch_token_id([src],len(src.split(' ')))\n",
    "        #print(\"src_id:\",src_id)\n",
    "        trg_id = self.tokenizer.en_token_id([trg],len(trg.split(' '))+2)\n",
    "        #print(\"trg_id:\",trg_id)\n",
    "        src_tensor = torch.LongTensor(src_id)\n",
    "        trg_tensor = torch.LongTensor(trg_id)\n",
    "        \n",
    "        b = Transformer.Batch(src_tensor,trg_tensor)\n",
    "        \n",
    "        trg = b.trg\n",
    "        copy_times = trg.shape[1]\n",
    "        #print(copy_times)\n",
    "        trg = trg.repeat(copy_times,1)\n",
    "        src = b.src\n",
    "        #print(src.shape)\n",
    "        src = src.repeat(copy_times,1)\n",
    "        #print(src.shape)\n",
    "        src_mask = b.src_mask\n",
    "        #print(src_mask.shape)\n",
    "        src_mask = src_mask.repeat(copy_times,1,1)\n",
    "        trg_mask = b.trg_mask\n",
    "        trg_y = b.trg_y.repeat(copy_times,1)\n",
    "        trg_y = trg_y.masked_fill(Transformer.subsequent_mask(copy_times)==0,0)\n",
    "        return src.to(device),src_mask.to(device),trg.to(device),trg_mask.to(device),trg_y.to(device),b.ntokens.to(device)\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397fcf39-0378-4c07-8e7c-524ec5de4923",
   "metadata": {},
   "source": [
    "## DataSet以及Tokenizer初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4254422-65fb-454e-b18a-267c2229ddb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文字典字数 3643\n",
      "英文字典字数 8349\n"
     ]
    }
   ],
   "source": [
    "#路径设定 请设定为自己的路径\n",
    "train_path = '/home/jovyan/input/anki2023_en_ch/train.txt'\n",
    "data_path = '/home/jovyan/input/anki2023_en_ch/cmn.txt'\n",
    "#初始化Tokenizer en为目标语言\n",
    "tokenizer = Mytokenizer(data_path,'en')\n",
    "dataset = Mydataset(train_path,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b91d4d-e01c-45c3-bda3-6f5ac5aaac77",
   "metadata": {},
   "source": [
    "## 模型、优化函数、损失函数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e42d5b3-282a-4c8b-a7ee-5e4a878f4b2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "#获取中文词典、和英文词典的大小\n",
    "src_vocab,trg_vocab = tokenizer.get_vocab()\n",
    "d_model = 512\n",
    "#获得Transformer对象\n",
    "model = Transformer.make_model(src_vocab,trg_vocab,N=3,d_model=d_model,d_ff=2048,h=8).to(device)\n",
    "#Transformer生成器\n",
    "generater = Transformer.Generator(d_model,trg_vocab).to(device)\n",
    "#自定义优化函数对象\n",
    "opt = Transformer.get_std_opt(model)\n",
    "label_smoothing = Transformer.LabelSmoothing(trg_vocab,0,0.1)\n",
    "loss_compute = Transformer.SimpleLossCompute(generater,label_smoothing,opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2773a3-424d-49d5-8195-dcdec57c694b",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cec75b37-812d-4450-bfdb-fe4f7fc55f5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(22.4320)\n",
      "loss: tensor(23.9844)\n",
      "loss: tensor(24.3766)\n",
      "loss: tensor(22.7481)\n",
      "loss: tensor(26.2118)\n",
      "loss: tensor(25.8319)\n",
      "loss: tensor(24.6913)\n",
      "loss: tensor(24.7470)\n",
      "loss: tensor(48.8272)\n",
      "loss: tensor(50.4420)\n",
      "loss: tensor(45.6334)\n",
      "loss: tensor(21.9424)\n",
      "loss: tensor(48.7035)\n",
      "loss: tensor(49.0590)\n",
      "loss: tensor(48.9000)\n",
      "loss: tensor(45.7834)\n",
      "loss: tensor(45.3217)\n",
      "loss: tensor(46.4886)\n",
      "loss: tensor(45.5641)\n",
      "loss: tensor(47.5375)\n",
      "loss: tensor(48.2569)\n",
      "loss: tensor(46.6815)\n",
      "loss: tensor(25.3270)\n",
      "loss: tensor(46.6383)\n",
      "loss: tensor(48.0010)\n",
      "loss: tensor(22.4841)\n",
      "loss: tensor(22.2173)\n",
      "loss: tensor(48.3282)\n",
      "loss: tensor(44.2221)\n",
      "loss: tensor(48.9370)\n",
      "loss: tensor(44.5197)\n",
      "loss: tensor(22.1568)\n",
      "loss: tensor(45.9320)\n",
      "loss: tensor(46.6134)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m e_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m src,src_mask,trg,trg_mask,trg_y,tokens \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#数据展示\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#print(src.shape,'-',src_mask.shape,'    ',trg.shape,'-',trg_mask.shape,'**',trg_y.shape)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(src,trg,src_mask,trg_mask)\n\u001b[1;32m      9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_compute(output,trg_y,tokens)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     10\u001b[0m     e_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mloss\n",
      "File \u001b[0;32m~/work/Transformer.py:52\u001b[0m, in \u001b[0;36mEncoderDecoder.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, tgt, src_mask, tgt_mask):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m#简单封装,参数顺序改变了\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(src, src_mask), src_mask,\n\u001b[1;32m     53\u001b[0m                         tgt, tgt_mask)\n",
      "File \u001b[0;32m~/work/Transformer.py:57\u001b[0m, in \u001b[0;36mEncoderDecoder.encode\u001b[0;34m(self, src, src_mask)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, src_mask):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m#对数据进行整理\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_embed(src), src_mask)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/Transformer.py:115\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m将数据和mask重复输入每一层\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 115\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x, mask)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/Transformer.py:181\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[0;32m--> 181\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublayer[\u001b[38;5;241m0\u001b[39m](x, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(x, x, x, mask))\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublayer[\u001b[38;5;241m1\u001b[39m](x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/Transformer.py:161\u001b[0m, in \u001b[0;36mSublayerConnection.forward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, sublayer):\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m#sublayer需为实例化的神经网络结构，并且可以保证输入和输出的维度保持一致\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sublayer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)))\n",
      "File \u001b[0;32m~/work/Transformer.py:181\u001b[0m, in \u001b[0;36mEncoderLayer.forward.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[0;32m--> 181\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublayer[\u001b[38;5;241m0\u001b[39m](x, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(x, x, x, mask))\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublayer[\u001b[38;5;241m1\u001b[39m](x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/Transformer.py:295\u001b[0m, in \u001b[0;36mMultiHeadedAttention.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m    290\u001b[0m nbatches \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# 先分别将QKV分别输入线性回归中，然后将得到的QKV分别将维度划分多头\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m#shape=(batch,seqlen,d_model)->Linear(维度不变)->(batch,head,seqlen,d_k)\u001b[39;00m\n\u001b[1;32m    294\u001b[0m query, key, value \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m--> 295\u001b[0m     [l(x)\u001b[38;5;241m.\u001b[39mview(nbatches, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    296\u001b[0m      \u001b[38;5;28;01mfor\u001b[39;00m l, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinears, (query, key, value))]\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# 将分好头的数据输入Attention，并得出结果\u001b[39;00m\n\u001b[1;32m    299\u001b[0m x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn \u001b[38;5;241m=\u001b[39m attention(query, key, value, mask\u001b[38;5;241m=\u001b[39mmask, \n\u001b[1;32m    300\u001b[0m                          dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout)\n",
      "File \u001b[0;32m~/work/Transformer.py:295\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    290\u001b[0m nbatches \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# 先分别将QKV分别输入线性回归中，然后将得到的QKV分别将维度划分多头\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m#shape=(batch,seqlen,d_model)->Linear(维度不变)->(batch,head,seqlen,d_k)\u001b[39;00m\n\u001b[1;32m    294\u001b[0m query, key, value \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m--> 295\u001b[0m     [l(x)\u001b[38;5;241m.\u001b[39mview(nbatches, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    296\u001b[0m      \u001b[38;5;28;01mfor\u001b[39;00m l, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinears, (query, key, value))]\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# 将分好头的数据输入Attention，并得出结果\u001b[39;00m\n\u001b[1;32m    299\u001b[0m x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn \u001b[38;5;241m=\u001b[39m attention(query, key, value, mask\u001b[38;5;241m=\u001b[39mmask, \n\u001b[1;32m    300\u001b[0m                          dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#训练次数\n",
    "epoch = 3\n",
    "for i in range(epoch):\n",
    "    e_loss = 0\n",
    "    for src,src_mask,trg,trg_mask,trg_y,tokens in dataset:\n",
    "        #数据展示\n",
    "        #print(src.shape,'-',src_mask.shape,'    ',trg.shape,'-',trg_mask.shape,'**',trg_y.shape)\n",
    "        output = model.forward(src,trg,src_mask,trg_mask)\n",
    "        loss = loss_compute(output,trg_y,tokens).cpu()\n",
    "        e_loss+=loss\n",
    "        print('loss:',loss)\n",
    "    print(f\"代数：{i+1} , 损失:{e_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78bb43c-abd1-42e5-90ef-165bc6f3ca60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f6f7e-eb75-42a5-ab99-40927425594c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch2.1.0",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
