{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d67ae512",
   "metadata": {},
   "source": [
    "针对文本中使用词的数量远远小于Bert的tokenizer的数量时，\\\n",
    "使用本函数可以有效减少embedding的参数量\\\n",
    "\n",
    "实现了一个类Mytokenizer\\\n",
    "会在实例化是生成中文和英文id字典，并提供token-id功能，并且添加起始控制符和填充符\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "def478b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84658eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建一个字典，包含控制符<BOS><EOS><PAD>\n",
    "def MakeDictWithControlCharacter(weather_control):\n",
    "    #是否加入除<pad>之外的控制符\n",
    "    dic = {}\n",
    "    dic['<PAD>']=0\n",
    "    if weather_control:\n",
    "        dic['<BOS>']=1\n",
    "        dic['<EOS>']=2\n",
    "    return dic,len(dic)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d12e6102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ch_dic(path,weather_control):\n",
    "    \"\"\"\n",
    "    path@ 全文本的txt路径\n",
    "    weather_control@ 是否加入控制符 如果为True会在字典最前面添加pad=0 bos=1 eos=2\n",
    "    功能是遍历所有中文文本，将所有字映射一个id\n",
    "    \"\"\"\n",
    "    text_path = path\n",
    "    with open(text_path,'r',encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "        #cleaned_ch = re.sub(\"[,|,|.|。|?|？|！|!]\", '', line[1])\n",
    "    all_word_list = []\n",
    "    for line in lines:\n",
    "        ch = line.split('\\t')[1]\n",
    "        cleaned_ch = re.sub(\"[,|,|.|。|?|？|！|!]\", '', ch)\n",
    "        all_word_list.extend(list(cleaned_ch))\n",
    "    #print(len(all_word_list))\n",
    "    dic,count = MakeDictWithControlCharacter(weather_control)\n",
    "    for word in all_word_list:\n",
    "        if word not in dic:\n",
    "            dic[word]=count\n",
    "            count+=1\n",
    "    print('中文字典字数',len(dic))\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fac88128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_en_dic(path,weather_control):\n",
    "    \"\"\"\n",
    "    path@ 全文本的txt路径\n",
    "    weather_control@ 是否加入控制符\n",
    "    功能是遍历所有英文文本，将所有字映射一个id\n",
    "    \"\"\"\n",
    "    with open(path,'r',encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "    all_word_list = []\n",
    "    for line in lines:\n",
    "        en = line.split('\\t')[0]\n",
    "        cleaned_en = re.sub(\"[,|.|!|?]\",'',en)\n",
    "        word_list = en.split(' ')\n",
    "        all_word_list.extend(word_list)\n",
    "    #print(len(all_word_list))\n",
    "    dic,count = MakeDictWithControlCharacter(weather_control)\n",
    "    for word in all_word_list:\n",
    "        if word not in dic:\n",
    "            dic[word]=count\n",
    "            count+=1\n",
    "    print('英文字典字数',len(dic))\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4792b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mytokenizer():\n",
    "    def __init__(self,path,tgr='en'):\n",
    "        #pad的id为0,bos 1,eos 2\n",
    "        self.ch_dic = make_ch_dic(path,True)\n",
    "        self.en_dic = make_en_dic(path,True)\n",
    "        self.id_ch_dic = {v:k for k,v in self.ch_dic.items()}\n",
    "        self.id_en_dic = {v:k for k,v in self.en_dic.items()}\n",
    "        self.tgr=tgr\n",
    "        \n",
    "    def ch_token_id(self,tokens:list,max_len):\n",
    "        #输入应该是['词 用 空 格 分 开','句 子 用 列 表 隔 开']\n",
    "        if_tgr = self.tgr=='ch'\n",
    "        all_list=[]\n",
    "        #遍历所有句子\n",
    "        for sentence in tokens:\n",
    "            sentence_ids = []\n",
    "            #遍历所有词\n",
    "            for word in sentence.split(\" \"):\n",
    "                #将词转换为id并添加入句子id列表中\n",
    "                sentence_ids.append(self.ch_dic[word])\n",
    "            if if_tgr:\n",
    "                #如果该字典是tgr输入则添加控制符\n",
    "                sentence_ids = self.__add_control_element(sentence_ids)\n",
    "            #添加padding\n",
    "            sentence_ids = self.__add_pad(sentence_ids,max_len)\n",
    "            all_list.append(sentence_ids)\n",
    "        return all_list\n",
    "    \n",
    "    #中文id转token\n",
    "    def ch_id_token(self,id_list:list):\n",
    "        tokens_list = []\n",
    "        for ids in id_list:\n",
    "            tokens = []\n",
    "            for id in ids:\n",
    "                tokens.append(self.id_ch_dic[id])\n",
    "            tokens_list.append(tokens)\n",
    "        return tokens_list\n",
    "    \n",
    "    #英文id转token\n",
    "    def en_id_token(self,id_list:list):\n",
    "        tokens_list = []\n",
    "        for ids in id_list:\n",
    "            tokens = []\n",
    "            for id in ids:\n",
    "                tokens.append(self.id_en_dic[id])\n",
    "            tokens_list.append(tokens)\n",
    "        return tokens_list\n",
    "    \n",
    "    def en_token_id(self,tokens:list,max_len):\n",
    "        #输入应该是['word distinct by space','sentence split with list']\n",
    "        if_tgr = self.tgr=='en'\n",
    "        all_list=[]\n",
    "        for sentence in tokens:\n",
    "            sentence_ids = []\n",
    "            for word in sentence.split(\" \"):\n",
    "                sentence_ids.append(self.en_dic[word])\n",
    "            if if_tgr:\n",
    "                sentence_ids = self.__add_control_element(sentence_ids)\n",
    "            sentence_ids = self.__add_pad(sentence_ids,max_len)\n",
    "            all_list.append(sentence_ids)\n",
    "        return all_list\n",
    "    \n",
    "    #为一句已经id化的列表加控制符\n",
    "    def __add_control_element(self,ids:list):\n",
    "        #输入应该为[5,6,7,4,9]\n",
    "        #输出为[1,5,6,7,4,9,2]\n",
    "        #插入<BOS>\n",
    "        processed_ids = ids.copy()\n",
    "        processed_ids.insert(0,1)\n",
    "        #插入<EOS>\n",
    "        processed_ids.append(2)\n",
    "        return processed_ids\n",
    "    \n",
    "    #为一句已经id化并添加控制符后的列表添加填充符\n",
    "    def __add_pad(self,ids:list,max_len):\n",
    "        #输入超过了最大长度\n",
    "        length = len(ids)\n",
    "        #如果已经超过最大长度报错\n",
    "        assert length<=max_len\n",
    "        paded_ids = ids.copy()\n",
    "        if length==max_len:\n",
    "            return paded_ids\n",
    "        while len(paded_ids)<max_len:\n",
    "            paded_ids.append(0)\n",
    "        return paded_ids\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ce5df",
   "metadata": {},
   "source": [
    "使用方式展示，请生成python文件，并在主函数中调用，以下为使用演示，请勿直接在这使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "db24e93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文字典字数 3643\n",
      "英文字典字数 12630\n",
      "[[195, 2204, 203, 4, 21, 183, 54, 0, 0, 0], [16, 21, 169, 1666, 170, 0, 0, 0, 0, 0]]\n",
      "[[1, 300, 271, 321, 654, 508, 2, 0, 0, 0], [1, 644, 9, 302, 2, 0, 0, 0, 0, 0]]\n",
      "[['这', '句', '话', '你', '会', '说', '么', '<PAD>', '<PAD>', '<PAD>'], ['我', '会', '个', '锤', '子', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']]\n",
      "[['<BOS>', 'can', 'you', 'do', 'this', 'one', '<EOS>', '<PAD>', '<PAD>', '<PAD>'], ['<BOS>', 'no', 'I', \"can't\", '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']]\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    tokenizer = Mytokenizer('./cmn.txt','en')\n",
    "    #句子转id\n",
    "    id_ch = tokenizer.ch_token_id([\"这 句 话 你 会 说 么\",\"我 会 个 锤 子\"],10)\n",
    "    id_en = tokenizer.en_token_id([\"can you do this one\",\"no I can't\"],10)\n",
    "    print(id_ch)\n",
    "    print(id_en)\n",
    "    #id转句子\n",
    "    tokens_ch = tokenizer.ch_id_token(id_ch)\n",
    "    tokens_en = tokenizer.en_id_token(id_en)\n",
    "    print(tokens_ch)\n",
    "    print(tokens_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3741749c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
