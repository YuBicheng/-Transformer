{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe0bce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utilities import Mytokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8357a97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文字典字数 3643\n",
      "英文字典字数 8349\n"
     ]
    }
   ],
   "source": [
    "train_path = './train.txt'\n",
    "data_path = './cmn.txt'\n",
    "tokenizer = Mytokenizer(data_path,'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d5f9c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mydataset(Dataset):\n",
    "    \"\"\"\n",
    "    @file_path 数据存储位置\n",
    "    @tokenizer 将文字id化的实例化后的Tokenizer\n",
    "    @文件中 trg数据的位置\n",
    "    \n",
    "    由于Decoder的输入要求，一句话应被切分成多段，目标词数有多少就应切分多少次\n",
    "    那么用一个自己写在Transformer中的batch类封装一句话，会自动的给句子mask，\n",
    "    并且切分Decoder的输入输出,但是注意例如:\n",
    "    trg为 \"I love food\" 那么应该输入四次，输出四次\n",
    "        输入                   输出\n",
    "    <BOS> mask mask mask   I \n",
    "    <BOS> I    mask mask   I love\n",
    "    <BOS> I    love mask   I love food\n",
    "    <BOS> I    love food   I love food <EOS>\n",
    "    \n",
    "    文件中的样子如下，中译英任务英文为目标语言 trg_index应为0\n",
    "    Hi\t嗨\n",
    "    Hi\t你 好\n",
    "    Run\t你 用 跑 的\n",
    "    \"\"\"\n",
    "    def __init__(self,file_path,tokenizer,trg_index=0):\n",
    "        self.tokenizer = tokenizer\n",
    "        #读取所有文本\n",
    "        with open(file_path,'r',encoding='utf8') as f:\n",
    "            self.lines = f.readlines()\n",
    "        #self.trg_count_words_line = []\n",
    "        self.length = len(self.lines)\n",
    "        self.trg_index = trg_index\n",
    "        if trg_index==0:\n",
    "            self.src_index = 1\n",
    "        else:\n",
    "            self.src_index = 0\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        src = self.lines[index].split('\\t')[self.src_index]\n",
    "        src = src.split('\\n')[0]\n",
    "        print(\"src:\",src)\n",
    "        trg = self.lines[index].split('\\t')[self.trg_index]\n",
    "        print(\"trg:\",trg)\n",
    "        #如上面的例子 三个词的句子应有四个样本,所以应该拷贝三次\n",
    "        copy_time = len(trg.split(' '))\n",
    "        print(copy_time)\n",
    "        # src_id化 这里简单定义了src使用中文\n",
    "        src_id = self.tokenizer.ch_token_id([src],len(src.split(' ')))\n",
    "        print(\"src_id:\",src_id)\n",
    "        trg_id = self.tokenizer.en_token_id([trg],len(trg.split(' '))+2)\n",
    "        print(\"trg_id:\",trg_id)\n",
    "        src_tensor = torch.LongTensor(src_id)\n",
    "        trg_tensor = torch.LongTensor(trg_id)\n",
    "        print('src_tensor:',src_tensor.shape,'trg_tensor:',trg_tensor.shape)\n",
    "        #复制   \n",
    "        #src_tensor = src_tensor.repeat(copy_time+1,1)\n",
    "        #trg_tensor = trg_tensor.repeat(copy_time+1,1)\n",
    "        #print(src_tensor)\n",
    "        #print(trg_tensor)\n",
    "        b = Transformer.Batch(src_tensor,trg_tensor)\n",
    "        print('数据最终形态')\n",
    "        print('输入',b.trg)\n",
    "        print('输出',b.trg_y)\n",
    "        print('mask',b.trg_mask)\n",
    "        return b\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6610eca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23635\n",
      "src: 我 真 蠢\n",
      "trg: I'm so stupid\n",
      "3\n",
      "src_id: [[16, 226, 309]]\n",
      "trg_id: [[1, 23, 175, 698, 2]]\n",
      "src_tensor: torch.Size([1, 3]) trg_tensor: torch.Size([1, 5])\n",
      "数据最终形态\n",
      "输入 tensor([[  1,  23, 175, 698]])\n",
      "输出 tensor([[ 23, 175, 698,   2]])\n",
      "mask tensor([[[ True, False, False, False],\n",
      "         [ True,  True, False, False],\n",
      "         [ True,  True,  True, False],\n",
      "         [ True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "dataset = Mydataset(train_path,tokenizer)\n",
    "print(len(dataset))\n",
    "b1 = dataset.__getitem__(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce252e",
   "metadata": {},
   "source": [
    "将要测试输入输出的模块 实例化步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2a72586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YBC\\jupyter\\Transformer.py:398: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    }
   ],
   "source": [
    "src_vocab,trg_vocab = tokenizer.get_vocab()\n",
    "d_model = 512\n",
    "#这是一个简单的Transformer网络，只有一层encoder decoder 注：无生成器\n",
    "model = Transformer.make_model(src_vocab,trg_vocab,1,d_model)\n",
    "en_embedding = Transformer.Embeddings(d_model,trg_vocab)\n",
    "ch_embedding = Transformer.Embeddings(d_model,src_vocab)\n",
    "multihead_attention = Transformer.MultiHeadedAttention(8,d_model)\n",
    "pe = Transformer.PositionalEncoding(d_model,0.1,max_len=20)\n",
    "generater = Transformer.Generator(d_model,trg_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cec520b",
   "metadata": {},
   "source": [
    "## 数据模拟\n",
    "通过未训练的网络运行数据获得对应的数据形状\\\n",
    "顺序为Embedding->PositionEncoding->MultiheadAttention\\\n",
    "Encoder-MultiHead 输入来源为q:src_tensor k:src_tensor v:src_tensor\\\n",
    "Dncoder-MultiHead\\\n",
    "输入来源为\\\n",
    "1&emsp;q:trg_tensor&emsp;k:trg_tensor&emsp;&emsp;&emsp;v:trg_tensor&emsp;mask:trg_mask\\\n",
    "2&emsp;q:trg_tensor&emsp;k:encoder_output&emsp;v:encoder_output&emsp;mask:src_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca5ebf8e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding： torch.Size([1, 3, 512]) torch.Size([1, 4, 512])\n",
      "PositionEncoding: torch.Size([1, 3, 512]) torch.Size([1, 4, 512])\n",
      "torch.Size([1, 1, 3])\n",
      "Encoder-Multihead: torch.Size([1, 3, 512])\n",
      "Decoder-masked-Multihead: torch.Size([1, 4, 512])\n",
      "这里开始完整Transformer结构数据流程\n",
      "输入模型的数据为(其实就是一句话复制了四遍):\n",
      " tensor([[ 16, 226, 309],\n",
      "        [ 16, 226, 309],\n",
      "        [ 16, 226, 309],\n",
      "        [ 16, 226, 309]]) \n",
      " tensor([[  1,  23, 175, 698],\n",
      "        [  1,  23, 175, 698],\n",
      "        [  1,  23, 175, 698],\n",
      "        [  1,  23, 175, 698]])\n",
      "Total Model: torch.Size([4, 4, 512])\n",
      "Generater: torch.Size([4, 4, 8349])\n",
      "目前我们已经获得了四句话，四个词的id了,当然这四句应该是\n",
      "I'm |<PAD>|<PAD>|<PAD>\n",
      "I'm |so | <PAD> |<PAD>\n",
      "I'm |so |stupid |<PAD>\n",
      "I'm |so |stupid |<EOS>\n",
      "那么我们需要将这四句话的id化的向量与generator的输出计算损失\n",
      "但这里需要注意，每次只计算一个词的损失，如第一行应计算I'm\n",
      "第二行应计算so 以此类推 其他词的差距不进行计算\n"
     ]
    }
   ],
   "source": [
    "#这里是模拟一个样本输入Transformer内部数据处理流程，并且打印从各个模块出来的数据形状\n",
    "src_tensor = ch_embedding(b1.src)\n",
    "trg_tensor = en_embedding(b1.trg)\n",
    "print(\"Embedding：\",src_tensor.shape,trg_tensor.shape)\n",
    "src_tensor = pe(src_tensor)\n",
    "trg_tensor = pe(trg_tensor)\n",
    "print(\"PositionEncoding:\",src_tensor.shape,trg_tensor.shape)\n",
    "print(b1.src_mask.shape)\n",
    "encoder_output = multihead_attention(src_tensor,src_tensor,src_tensor,b1.src_mask)\n",
    "print('Encoder-Multihead:',encoder_output.shape)\n",
    "decoder_output = multihead_attention(trg_tensor,trg_tensor,trg_tensor,b1.trg_mask)\n",
    "decoder_output = multihead_attention(trg_tensor,encoder_output,encoder_output,b1.src_mask)\n",
    "print('Decoder-masked-Multihead:',decoder_output.shape)\n",
    "print(\"这里开始完整Transformer结构数据流程\")\n",
    "#这里是直接将数据输入Transformer 与上面代码无关\n",
    "trg_input = b1.trg\n",
    "copy_time = trg_input.shape[1]\n",
    "#print(trg_input.shape)\n",
    "trg_input = trg_input.repeat(copy_time,1)\n",
    "src_input = b1.src\n",
    "src_input = src_input.repeat(copy_time,1)\n",
    "print('输入模型的数据为(其实就是一句话复制了四遍):\\n',src_input,'\\n',trg_input)\n",
    "transformer_output = model(src_input,trg_input,b1.src_mask,b1.trg_mask)\n",
    "print(\"Total Model:\",transformer_output.shape)\n",
    "generater_output = generater(transformer_output)\n",
    "print(\"Generater:\",generater_output.shape)\n",
    "print(\"目前我们已经获得了四句话，四个词的id了,当然这四句应该是\")\n",
    "print(\"I'm |<PAD>|<PAD>|<PAD>\")\n",
    "print(\"I'm |so | <PAD> |<PAD>\")\n",
    "print(\"I'm |so |stupid |<PAD>\")\n",
    "print(\"I'm |so |stupid |<EOS>\")\n",
    "print(\"那么我们需要将这四句话的id化的向量与generator的输出计算损失\")\n",
    "print(\"但这里需要注意，每次只计算一个词的损失，如第一行应计算I'm\\n\\\n",
    "第二行应计算so 以此类推 其他词的差距不进行计算\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2725661c",
   "metadata": {},
   "source": [
    "## mask 展示\n",
    "下面展示以下mask后的decoder输入\n",
    "这个操作其实是在attention中执行的，这里只是简单展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90e4ce02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[          1, -1000000000, -1000000000, -1000000000],\n",
       "         [          1,          23, -1000000000, -1000000000],\n",
       "         [          1,          23,         175, -1000000000],\n",
       "         [          1,          23,         175,         698]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_input.masked_fill(b1.trg_mask==0,-1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a009a2f",
   "metadata": {},
   "source": [
    "## 以上为一条数据全部流程\n",
    "这里没有演示损失函数计算以及优化过程，损失函数哈佛学习版使用了KL散度，而attention论文中使用了交叉熵，这里不做评价，请自行选择合适的损失函数\\\n",
    "Transformer的优化是带有热身的，1个小时连热身都跑不完，所以就算了\\\n",
    "一条数据的如何处理如何走过整个模型的样子已经演示了\\\n",
    "相信你肯定会批量训练了吧🤡\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
