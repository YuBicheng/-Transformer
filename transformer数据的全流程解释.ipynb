{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe0bce4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Utilities import Mytokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy,math\n",
    "import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8357a97a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文字典字数 3643\n",
      "英文字典字数 8349\n"
     ]
    }
   ],
   "source": [
    "train_path = '/home/jovyan/input/anki2023_en_ch/train.txt'\n",
    "data_path = '/home/jovyan/input/anki2023_en_ch/cmn.txt'\n",
    "tokenizer = Mytokenizer(data_path,'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd19d8e-f755-4949-a49c-28f92bc7ca26",
   "metadata": {},
   "source": [
    "## 自定义的数据集\n",
    "这个数据集实现了将文字转id 并且每个过程都有对应的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d5f9c81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Mydataset(Dataset):\n",
    "    \"\"\"\n",
    "    @file_path 数据存储位置\n",
    "    @tokenizer 将文字id化的实例化后的Tokenizer\n",
    "    @文件中 trg数据的位置\n",
    "    \n",
    "    由于Decoder的输入要求，一句话应被切分成多段，目标词数有多少就应切分多少次\n",
    "    那么用一个自己写在Transformer中的batch类封装一句话，会自动的给句子mask，\n",
    "    并且切分Decoder的输入输出,但是注意例如:\n",
    "    trg为 \"I love food\" 那么应该输入四次，输出四次\n",
    "        输入                   输出\n",
    "    <BOS> mask mask mask   I \n",
    "    <BOS> I    mask mask   I love\n",
    "    <BOS> I    love mask   I love food\n",
    "    <BOS> I    love food   I love food <EOS>\n",
    "    \n",
    "    文件中的样子如下，中译英任务英文为目标语言 trg_index应为0\n",
    "    Hi\t嗨\n",
    "    Hi\t你 好\n",
    "    Run\t你 用 跑 的\n",
    "    \"\"\"\n",
    "    def __init__(self,file_path,tokenizer,trg_index=0):\n",
    "        self.tokenizer = tokenizer\n",
    "        #读取所有文本\n",
    "        with open(file_path,'r',encoding='utf8') as f:\n",
    "            self.lines = f.readlines()\n",
    "        #self.trg_count_words_line = []\n",
    "        self.length = len(self.lines)\n",
    "        self.trg_index = trg_index\n",
    "        if trg_index==0:\n",
    "            self.src_index = 1\n",
    "        else:\n",
    "            self.src_index = 0\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        src = self.lines[index].split('\\t')[self.src_index]\n",
    "        src = src.split('\\n')[0]\n",
    "        print(\"src:\",src)\n",
    "        trg = self.lines[index].split('\\t')[self.trg_index]\n",
    "        print(\"trg:\",trg)\n",
    "        #如上面的例子 三个词的句子应有四个样本,所以应该拷贝三次\n",
    "        copy_time = len(trg.split(' '))\n",
    "        print(copy_time)\n",
    "        # src_id化 这里简单定义了src使用中文\n",
    "        src_id = self.tokenizer.ch_token_id([src],len(src.split(' ')))\n",
    "        print(\"src_id:\",src_id)\n",
    "        trg_id = self.tokenizer.en_token_id([trg],len(trg.split(' '))+2)\n",
    "        print(\"trg_id:\",trg_id)\n",
    "        src_tensor = torch.LongTensor(src_id)\n",
    "        trg_tensor = torch.LongTensor(trg_id)\n",
    "        print('src_tensor:',src_tensor.shape,'trg_tensor:',trg_tensor.shape)\n",
    "        #复制   \n",
    "        #src_tensor = src_tensor.repeat(copy_time+1,1)\n",
    "        #trg_tensor = trg_tensor.repeat(copy_time+1,1)\n",
    "        #print(src_tensor)\n",
    "        #print(trg_tensor)\n",
    "        b = Transformer.Batch(src_tensor,trg_tensor)\n",
    "        print('数据最终形态')\n",
    "        print('输入',b.trg)\n",
    "        print('输出',b.trg_y)\n",
    "        print('mask',b.trg_mask)\n",
    "        return b\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6610eca7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23635\n",
      "src: 我 真 蠢\n",
      "trg: I'm so stupid\n",
      "3\n",
      "src_id: [[16, 226, 309]]\n",
      "trg_id: [[1, 23, 175, 698, 2]]\n",
      "src_tensor: torch.Size([1, 3]) trg_tensor: torch.Size([1, 5])\n",
      "数据最终形态\n",
      "输入 tensor([[  1,  23, 175, 698]])\n",
      "输出 tensor([[ 23, 175, 698,   2]])\n",
      "mask tensor([[[ True, False, False, False],\n",
      "         [ True,  True, False, False],\n",
      "         [ True,  True,  True, False],\n",
      "         [ True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "dataset = Mydataset(train_path,tokenizer)\n",
    "print(len(dataset))\n",
    "b1 = dataset.__getitem__(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce252e",
   "metadata": {},
   "source": [
    "将要测试输入输出的模块 实例化步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2a72586",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_vocab,trg_vocab = tokenizer.get_vocab()\n",
    "d_model = 512\n",
    "#这是一个简单的Transformer网络，只有一层encoder decoder 注：无生成器\n",
    "model = Transformer.make_model(src_vocab,trg_vocab,1,d_model)\n",
    "en_embedding = Transformer.Embeddings(d_model,trg_vocab)\n",
    "ch_embedding = Transformer.Embeddings(d_model,src_vocab)\n",
    "multihead_attention = Transformer.MultiHeadedAttention(8,d_model)\n",
    "pe = Transformer.PositionalEncoding(d_model,0.1,max_len=20)\n",
    "generater = Transformer.Generator(d_model,trg_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d5a971",
   "metadata": {},
   "source": [
    "## Transformer各个结构的输入输出模拟\n",
    "通过未训练的网络运行数据获得对应的数据形状\\\n",
    "顺序为Embedding->PositionEncoding->MultiheadAttention\\\n",
    "Encoder-MultiHead 输入来源为q:src_tensor k:src_tensor v:src_tensor\\\n",
    "Dncoder-MultiHead\\\n",
    "输入来源为\\\n",
    "1&emsp;q:trg_tensor&emsp;k:trg_tensor&emsp;&emsp;&emsp;v:trg_tensor&emsp;mask:trg_mask\\\n",
    "2&emsp;q:trg_tensor&emsp;k:encoder_output&emsp;v:encoder_output&emsp;mask:src_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca5ebf8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding： torch.Size([1, 3, 512]) torch.Size([1, 4, 512])\n",
      "PositionEncoding: torch.Size([1, 3, 512]) torch.Size([1, 4, 512])\n",
      "torch.Size([1, 1, 3])\n",
      "Encoder-Multihead: torch.Size([1, 3, 512])\n",
      "Decoder-masked-Multihead: torch.Size([1, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "#这里是模拟一个样本输入Transformer内部数据处理流程，并且打印从各个模块出来的数据形状\n",
    "src_tensor = ch_embedding(b1.src)\n",
    "trg_tensor = en_embedding(b1.trg)\n",
    "print(\"Embedding：\",src_tensor.shape,trg_tensor.shape)\n",
    "src_tensor = pe(src_tensor)\n",
    "trg_tensor = pe(trg_tensor)\n",
    "print(\"PositionEncoding:\",src_tensor.shape,trg_tensor.shape)\n",
    "print(b1.src_mask.shape)\n",
    "encoder_output = multihead_attention(src_tensor,src_tensor,src_tensor,b1.src_mask)\n",
    "print('Encoder-Multihead:',encoder_output.shape)\n",
    "decoder_output = multihead_attention(trg_tensor,trg_tensor,trg_tensor,b1.trg_mask)\n",
    "decoder_output = multihead_attention(trg_tensor,encoder_output,encoder_output,b1.src_mask)\n",
    "print('Decoder-masked-Multihead:',decoder_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e928bc-a02b-4b73-a638-2153487ef66c",
   "metadata": {},
   "source": [
    "## Transformer整体输入输出\n",
    "这里模拟了一条数据应该如何输入Transformer，已经对应的输出应该怎么处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd6b12ca-441e-4de6-a2f2-bc6290211e00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入模型的数据为(其实就是一句话复制了四遍):\n",
      " tensor([[ 16, 226, 309],\n",
      "        [ 16, 226, 309],\n",
      "        [ 16, 226, 309],\n",
      "        [ 16, 226, 309]]) \n",
      " tensor([[  1,  23, 175, 698],\n",
      "        [  1,  23, 175, 698],\n",
      "        [  1,  23, 175, 698],\n",
      "        [  1,  23, 175, 698]])\n",
      "Total Model: torch.Size([4, 4, 512])\n",
      "Generater: torch.Size([4, 4, 8349])\n",
      "目前我们已经获得了四句话，四个词的id了,当然这四句应该是\n",
      "I'm |<PAD>|<PAD>|<PAD>\n",
      "I'm |so | <PAD> |<PAD>\n",
      "I'm |so |stupid |<PAD>\n",
      "I'm |so |stupid |<EOS>\n",
      "那么我们需要将这四句话的id化的向量与generator的输出计算损失\n",
      "但这里需要注意，每次只计算一个词的损失，如第一行应计算I'm\n",
      "第二行应计算so 以此类推 其他词的差距不进行计算\n"
     ]
    }
   ],
   "source": [
    "trg_input = b1.trg\n",
    "copy_time = trg_input.shape[1]\n",
    "#print(trg_input.shape)\n",
    "trg_input = trg_input.repeat(copy_time,1)\n",
    "src_input = b1.src\n",
    "src_input = src_input.repeat(copy_time,1)\n",
    "print('输入模型的数据为(其实就是一句话复制了四遍):\\n',src_input,'\\n',trg_input)\n",
    "transformer_output = model(src_input,trg_input,b1.src_mask,b1.trg_mask)\n",
    "print(\"Total Model:\",transformer_output.shape)\n",
    "generater_output = generater(transformer_output)\n",
    "print(\"Generater:\",generater_output.shape)\n",
    "print(\"目前我们已经获得了四句话，四个词的id了,当然这四句应该是\")\n",
    "print(\"I'm |<PAD>|<PAD>|<PAD>\")\n",
    "print(\"I'm |so | <PAD> |<PAD>\")\n",
    "print(\"I'm |so |stupid |<PAD>\")\n",
    "print(\"I'm |so |stupid |<EOS>\")\n",
    "print(\"那么我们需要将这四句话的id化的向量与generator的输出计算损失\")\n",
    "print(\"但这里需要注意，每次只计算一个词的损失，如第一行应计算I'm\\n\\\n",
    "第二行应计算so 以此类推 其他词的差距不进行计算\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63ad524-826f-4031-a18c-2736897cbbbd",
   "metadata": {},
   "source": [
    "## Multihead-Attention 内部数据处理的变化\n",
    "虽然Transformer中已经实现过一遍了，但这里为了方便展示，添加了一个每个步骤输出数据形状\\\n",
    "方便理解\\\n",
    "这里是代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "075040c5-c996-4a1e-8de1-adc99c2ebda0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Attention_display(query, key, value, mask=None, dropout=None):\n",
    "    \"普通的点积型注意力\"\n",
    "    #输入的QKV的维度为(batch,head,quelen,d_k)d_k为分头后的数据\n",
    "    d_k = query.size(-1)\n",
    "    #只对后两维进行矩阵乘法，保证多头注意力，各个头的数据隔离\n",
    "    print(\"A-这时需要进行QK相乘的操作 这时QK的维度为:\",query.shape,key.shape)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    print(\"A-这时已经完成QK相乘的操作，需要对score矩阵进行mask操作 这时Attention score的维度为:\",scores.shape)\n",
    "    if mask is not None:\n",
    "        #如果有掩码，则将数据替换为-1e9(接近无穷小)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    print(\"A-这里如果有掩码操作的话会进行掩码 这时的score维度未变只是数据一部分被遮掩掉了,不方便展示 请看下面的mask单独的演示\")\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    print(\"A-这里对最后一个维度的数据进行softmax操作,此时score的维度为:\",scores.shape)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    print(\"A-这里进行了dropout操作维度未发生改变\")\n",
    "    output = torch.matmul(p_attn, value)\n",
    "    print(\"A-这进行了将score与value相乘的过程 最终输出为:\",output.shape)\n",
    "    return output, p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"需要输入head数以及d_model,head需要可以整除d_model\"\n",
    "        \"多头的目的是让不同的头提取的特征不同，以丰富模型的特征提取\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # 算出分多头后的维度 如512维 八头维度变为(8,64)\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        #如果有mask操作在第一个维度后加入维度\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 先分别将QKV分别输入线性回归中，然后将得到的QKV分别将维度划分多头\n",
    "        #shape=(batch,seqlen,d_model)->Linear(维度不变)->(batch,head,seqlen,d_k)\n",
    "        query, key, value = [l(x) for l, x in zip(self.linears, (query, key, value))]\n",
    "        print(\"M-query先进入一个FNN，维度不改变，KV同理  这时query维度:\",query.shape)\n",
    "        query = query.view(nbatches, -1, self.h, self.d_k)\n",
    "        print(\"M-这时query进行了分头变换 这时的维度为:\",query.shape)\n",
    "        query = query.transpose(1, 2)\n",
    "        print(\"M-这时query进行了维度交换 这时的维度为:\",query.shape)\n",
    "        key = key.view(nbatches, -1, self.h, self.d_k)\n",
    "        key = key.transpose(1, 2)\n",
    "        value = value.view(nbatches, -1, self.h, self.d_k)\n",
    "        value = value.transpose(1, 2)\n",
    "        # 将分好头的数据输入Attention，并得出结果\n",
    "        print(\"M-这时将变形后的QKV以及mask输入Attention\")\n",
    "        x, self.attn = Attention_display(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) 将数据还原为原来的样子 shape=(batch,seqlen,d_model)\n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        print(\"M-这里进行了数据维度的还原 这时维度为:\",x.shape)\n",
    "        print(\"M-这里对最终的输出输入了一个FNN 维度未改变\")\n",
    "        return self.linears[-1](x)\n",
    "    \n",
    "def clones(module, N):\n",
    "    \"\"\"这个函数很有用，因为整个网络结构有多次重复结构，可以用这个函数对实例进行深拷贝，例如可以复制多个Encoder或Linear\"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7eedb75b-775e-4cf6-a8cf-da6a8187c2ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention 输入: torch.Size([4, 4, 512])\n",
      "对应维度为 (batch，词个数，d_model)\n",
      "M-query先进入一个FNN，维度不改变，KV同理  这时query维度: torch.Size([4, 4, 512])\n",
      "M-这时query进行了分头变换 这时的维度为: torch.Size([4, 4, 8, 64])\n",
      "M-这时query进行了维度交换 这时的维度为: torch.Size([4, 8, 4, 64])\n",
      "M-这时将变形后的QKV以及mask输入Attention\n",
      "A-这时需要进行QK相乘的操作 这时QK的维度为: torch.Size([4, 8, 4, 64]) torch.Size([4, 8, 4, 64])\n",
      "A-这时已经完成QK相乘的操作，需要对score矩阵进行mask操作 这时Attention score的维度为: torch.Size([4, 8, 4, 4])\n",
      "A-这里如果有掩码操作的话会进行掩码 这时的score维度未变只是数据一部分被遮掩掉了,不方便展示 请看下面的mask单独的演示\n",
      "A-这里对最后一个维度的数据进行softmax操作,此时score的维度为: torch.Size([4, 8, 4, 4])\n",
      "A-这里进行了dropout操作维度未发生改变\n",
      "A-这进行了将score与value相乘的过程 最终输出为: torch.Size([4, 8, 4, 64])\n",
      "M-这里进行了数据维度的还原 这时维度为: torch.Size([4, 4, 512])\n",
      "M-这里对最终的输出输入了一个FNN 维度未改变\n",
      "MultiHeadAttention输出 torch.Size([4, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "#实例化一个MultiHeadattention\n",
    "ma = MultiHeadedAttention(8,512)\n",
    "#数据使用前面\n",
    "input_data =  pe(en_embedding(trg_input))\n",
    "print('MultiHeadAttention 输入:',input_data.shape)\n",
    "print('对应维度为 (batch，词个数，d_model)')\n",
    "output = ma(input_data,input_data,input_data,b1.trg_mask)\n",
    "print('MultiHeadAttention输出',output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ba7f3",
   "metadata": {},
   "source": [
    "## mask 展示\n",
    "下面展示以下mask后的decoder输入\n",
    "这个操作其实是在attention中执行的，这里只是简单展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1cc12b65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1,  23, 175, 698],\n",
      "        [  1,  23, 175, 698],\n",
      "        [  1,  23, 175, 698],\n",
      "        [  1,  23, 175, 698]])\n",
      "-------------------------------------------------\n",
      "tensor([[[          1, -1000000000, -1000000000, -1000000000],\n",
      "         [          1,          23, -1000000000, -1000000000],\n",
      "         [          1,          23,         175, -1000000000],\n",
      "         [          1,          23,         175,         698]]])\n"
     ]
    }
   ],
   "source": [
    "print(trg_input)\n",
    "print(\"-------------------------------------------------\")\n",
    "print(trg_input.masked_fill(b1.trg_mask==0,-1e9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34680f4b",
   "metadata": {},
   "source": [
    "## 以上为一条数据全部流程\n",
    "这里没有演示损失函数计算以及优化过程，损失函数哈佛学习版使用了KL散度，而attention论文中使用了交叉熵，这里不做评价，请自行选择合适的损失函数\\\n",
    "Transformer的优化是带有热身的，1个小时连热身都跑不完，所以就算了\\\n",
    "一条数据的如何处理如何走过整个模型的样子已经演示了\\\n",
    "相信你肯定会批量训练了吧🤡\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch2.1.0",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
